


# STEP 1: Load libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns 
from collections import Counter
from sklearn.model_selection import train_test_split
from sklearn.impute import SimpleImputer
from imblearn.over_sampling import SMOTE
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC
from sklearn.naive_bayes import GaussianNB


# STEP 2: Load the preprocessed dataset
df = pd.read_excel("../data/Preprocessed_Diabetes_Data.xlsx")
df.head()


# STEP 3: Drop rows with missing essential features
df = df.dropna(subset=['Height_cm', 'Weight_kg'])


# STEP 4: Encode categorical variables
categorical_cols = ['Age', 'Gender', 'Marital_Status', 'Family_Income', 
                    'Education', 'Occupation', 'Residence', 'District']

for col in categorical_cols:
    df[col] = LabelEncoder().fit_transform(df[col].astype(str))


# STEP 5: Define features and target
features = ['Age', 'Gender', 'Marital_Status', 'Family_Income', 'Education',
            'Occupation', 'Residence', 'District', 'Gestational_Diabetes',
            'Family_History', 'High_Blood_Pressure', 'Physically_Active',
            'Height_cm', 'Weight_kg']

X = df[features]
y = df['Risk_Category'].map({'Low Risk (<5)': 0, 'High Risk (â‰¥5)': 1})


X.head()


y.head()


# STEP 6: Handle missing values with imputation (mean strategy)
imputer = SimpleImputer(strategy='mean')
X = pd.DataFrame(imputer.fit_transform(X), columns=features)


X.head()


# STEP 7: Check imbalance before SMOTE
print("Class distribution before SMOTE:", Counter(y))
sns.countplot(x=y)
plt.title("Class Distribution Before SMOTE")
plt.xticks([0, 1], ['Low Risk', 'High Risk'])
plt.ylabel("Count")
plt.tight_layout()
plt.show()


# STEP 8: Train-test split (before SMOTE)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)


# STEP 9: Feature scaling
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)


# STEP 10: Apply SMOTE to balance training set
smote = SMOTE(random_state=42)
X_train_balanced, y_train_balanced = smote.fit_resample(X_train_scaled, y_train)


# Visualize class distribution after SMOTE
print("Class distribution after SMOTE:", Counter(y_train_balanced))
sns.countplot(x=y_train_balanced)
plt.title("Class Distribution After SMOTE")
plt.xticks([0, 1], ['Low Risk', 'High Risk'])
plt.ylabel("Count")
plt.tight_layout()
plt.show()


# STEP 11: Train and evaluate multiple models
models = {
    "Random Forest": RandomForestClassifier(n_estimators=100, random_state=42),
    "Logistic Regression": LogisticRegression(max_iter=1000),
    "Decision Tree": DecisionTreeClassifier(),
    "Support Vector Machine": SVC(),
    "Naive Bayes": GaussianNB()
}

results = []

for name, model in models.items():
    model.fit(X_train_balanced, y_train_balanced)
    y_pred = model.predict(X_test_scaled)

    results.append({
        "Model": name,
        "Accuracy": accuracy_score(y_test, y_pred),
        "Precision": precision_score(y_test, y_pred),
        "Recall": recall_score(y_test, y_pred),
        "F1 Score": f1_score(y_test, y_pred)
    })

    print(f"\n{name}")
    print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred))
    print("Classification Report:\n", classification_report(y_test, y_pred))


# STEP 12: Compare model performance visually
results_df = pd.DataFrame(results).sort_values(by="F1 Score", ascending=False)
results_df


# Set a publication-style font and figure settings
plt.figure(figsize=(12, 7))
ax = results_df.set_index('Model')[['Accuracy', 'Precision', 'Recall', 'F1 Score']].plot(
    kind='bar', edgecolor='black', width=0.75, figsize=(12, 7)
)

plt.title("Model Performance Comparison", fontsize=16, weight='bold')
plt.ylabel("Score", fontsize=14)
plt.xlabel("Model", fontsize=14)
plt.ylim(0.7, 1.0)
plt.xticks(rotation=15, fontsize=12)
plt.yticks(fontsize=12)
plt.legend(title="Metric", title_fontsize=13, fontsize=12)
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.tight_layout()
plt.show()


# STEP 13: Train the model (Random Forest)
model = RandomForestClassifier(n_estimators=100, random_state=42)
model.fit(X_train_scaled, y_train)


# STEP 14: Evaluate the model
y_pred = model.predict(X_test_scaled)

print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred))
print("\nClassification Report:\n", classification_report(y_test, y_pred))


# Feature importance
importances = model.feature_importances_
indices = np.argsort(importances)[::-1]
plt.figure(figsize=(10, 6))
plt.title("Feature Importance")
plt.bar(range(len(features)), importances[indices])
plt.xticks(range(len(features)), [features[i] for i in indices], rotation=45)
plt.tight_layout()
plt.show()
